{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee72b1c",
   "metadata": {},
   "source": [
    "# ENEL 645 - Assignment 2\n",
    "## Group 30: Aneesh Bulusu, Long Nguyen, Strahinja Radakovic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f023df",
   "metadata": {},
   "source": [
    "In this assignment we were tasked with creating a machine learning model that can take an image and description as input, and output what kind of garbage disposal the imaged object should be subjected to.\n",
    "\n",
    "We attempted several different models (especially when it came to implementing the text data), but in the end the following model (very similar to the tutorials we went over in class) ended up performing the best, using ResNet50 for image classification and DistilBERT for text feature implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d9f4e",
   "metadata": {},
   "source": [
    "### Part 1: Set-up\n",
    "\n",
    "Here we import the necessary libraries for the model, and attempt to use a GPU to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf180a2",
   "metadata": {},
   "source": [
    "### Part 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb075ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the images to a standard size, converts them to tensors, and normalizes them.\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Tokenizing the text\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24\n",
    "\n",
    "\n",
    "# Extracting text from filenames\n",
    "def extract_text_from_filename(filename):\n",
    "    text = os.path.splitext(filename)[0]  # Remove extension\n",
    "    text = text.replace('_', ' ')  # Replace underscores\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    return text\n",
    "\n",
    "\n",
    "# Creating a custom dataset by merging the image and text data\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, image_dir, tokenizer, max_len, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.classes = sorted(os.listdir(image_dir))\n",
    "        self.label_map = {class_name: idx for idx, class_name in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        # Gathers image file paths and filenames\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(image_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file in os.listdir(class_path):\n",
    "                    self.samples.append((os.path.join(class_path, file), class_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, class_name = self.samples[idx]\n",
    "        label = self.label_map[class_name]\n",
    "\n",
    "        # Loads image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Processes text from filename\n",
    "        text = extract_text_from_filename(os.path.basename(image_path))\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Returns the image tensor, text tensor, attention mask tensor and label tensor\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ce3e3",
   "metadata": {},
   "source": [
    "### Part 3: Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd89e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting image features using ResNet50\n",
    "class ImageFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageFeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet50(weights=\"IMAGENET1K_V1\")\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Removing FC layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        return x.view(x.size(0), -1)  \n",
    "    \n",
    "    \n",
    "# Extracting text features using DistilBERT\n",
    "class TextFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextFeatureExtractor, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return output.last_hidden_state[:, 0, :] \n",
    "    \n",
    "\n",
    "# Combining image and text features together\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, image_feature_dim, text_feature_dim, num_classes):\n",
    "        super(MultimodalClassifier, self).__init__()\n",
    "\n",
    "        self.image_model = ImageFeatureExtractor()\n",
    "        self.text_model = TextFeatureExtractor()\n",
    "\n",
    "        # Combined feature size\n",
    "        combined_dim = image_feature_dim + text_feature_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.image_model(image)\n",
    "        text_features = self.text_model(input_ids, attention_mask)\n",
    "\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        output = self.classifier(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42369ded",
   "metadata": {},
   "source": [
    "### Part 4: Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00dcad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Defining paths to the datasets (the ones below are for the TALC cluster)\n",
    "TRAIN_PATH = \"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Train\"\n",
    "VAL_PATH = \"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Val\"\n",
    "TEST_PATH = \"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Test\"\n",
    "\n",
    "# Loading the datasets\n",
    "train_dataset = MultimodalDataset(TRAIN_PATH, tokenizer, max_len, transform=image_transform)\n",
    "val_dataset = MultimodalDataset(VAL_PATH, tokenizer, max_len, transform=image_transform)\n",
    "test_dataset = MultimodalDataset(TEST_PATH, tokenizer, max_len, transform=image_transform)\n",
    "\n",
    "# Createating loaders\n",
    "batch_size = 32  # in the future we could experiment more with different batch sizes to try to find the perfect balance between over- and under-fitting.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Defining the model\n",
    "image_feature_dim = 2048  # ResNet50 feature size\n",
    "text_feature_dim = 768  # DistilBERT CLS token size\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "model = MultimodalClassifier(image_feature_dim, text_feature_dim, num_classes).to(device)\n",
    "\n",
    "# Parameter Count\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "image_params = count_parameters(model.image_model)\n",
    "text_params = count_parameters(model.text_model)\n",
    "total_params = count_parameters(model)\n",
    "\n",
    "\n",
    "# Defining training parameters\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5) # in the future, we could look at finding the optimal learning rate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training/Validation loop\n",
    "epochs = 10  # in the future, we could look into the optimal number of epochs\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Script to run the epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Computing training accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = correct_train / total_train * 100\n",
    "\n",
    "    \n",
    "    \n",
    "    # VALIDATION! \n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = correct_val / total_val * 100 \n",
    "\n",
    "    # Print output\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save the best model\n",
    "    if avg_val_loss < best_loss:\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        best_loss = avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ade20",
   "metadata": {},
   "source": [
    "### Part 5: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc9b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the previously saved best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "# Loop over the test set to calculate test accuracy\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(images, input_ids, attention_mask)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_test += (preds == labels).sum().item()\n",
    "        total_test += labels.size(0)\n",
    "\n",
    "        test_predictions.extend(preds.cpu().numpy())  # Preparing for confusion matrix\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = correct_test / total_test * 100\n",
    "\n",
    "# Printing test accuracy\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a8cde",
   "metadata": {},
   "source": [
    "### Part 6: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a2c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "class_names = sorted(train_dataset.classes)  # Ensure correct class order\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Visualize first convolutional layer filters\n",
    "first_conv_weights = model.image_model.feature_extractor[0].weight.data.cpu()\n",
    "first_conv_weights = (first_conv_weights - first_conv_weights.min()) / (first_conv_weights.max() - first_conv_weights.min())\n",
    "\n",
    "# Plot filters\n",
    "plt.figure(figsize=(8, 8))\n",
    "grid = vutils.make_grid(first_conv_weights, normalize=True, nrow=8)\n",
    "plt.imshow(grid.permute(1, 2, 0))\n",
    "plt.title(\"First Convolutional Layer Filters\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"convolutional_filters.png\")\n",
    "plt.show()\n",
    "\n",
    "# Printing the number of parameters\n",
    "print(f\"Image Model (ResNet50) Parameters: {image_params:,}\")\n",
    "print(f\"Text Model (DistilBERT) Parameters: {text_params:,}\")\n",
    "print(f\"Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba369af",
   "metadata": {},
   "source": [
    "## Output\n",
    "*Warnings and prints that ended up being less useful are omitted. For full output check the slurm-34704.out file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1855b8",
   "metadata": {},
   "source": [
    "Epoch [1/10], Train Loss: 0.4898, Val Loss: 0.2942, Train Acc: 81.84%, Val Acc: 89.89%\n",
    "\n",
    "Epoch [2/10], Train Loss: 0.2211, Val Loss: 0.3102, Train Acc: 92.37%, Val Acc: 89.39%\n",
    "\n",
    "Epoch [3/10], Train Loss: 0.1081, Val Loss: 0.3342, Train Acc: 96.40%, Val Acc: 89.67%\n",
    "\n",
    "Epoch [4/10], Train Loss: 0.0469, Val Loss: 0.3917, Train Acc: 98.67%, Val Acc: 89.50%\n",
    "\n",
    "Epoch [5/10], Train Loss: 0.0245, Val Loss: 0.4075, Train Acc: 99.32%, Val Acc: 90.39%\n",
    "\n",
    "Epoch [6/10], Train Loss: 0.0159, Val Loss: 0.4867, Train Acc: 99.60%, Val Acc: 89.17%\n",
    "\n",
    "Epoch [7/10], Train Loss: 0.0141, Val Loss: 0.4723, Train Acc: 99.60%, Val Acc: 89.28%\n",
    "\n",
    "Epoch [8/10], Train Loss: 0.0220, Val Loss: 0.6345, Train Acc: 99.26%, Val Acc: 88.83%\n",
    "\n",
    "Epoch [9/10], Train Loss: 0.0280, Val Loss: 0.4908, Train Acc: 99.05%, Val Acc: 89.50%\n",
    "\n",
    "Epoch [10/10], Train Loss: 0.0207, Val Loss: 0.5048, Train Acc: 99.23%, Val Acc: 89.89%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Test Accuracy: 85.66%\n",
    "\n",
    "\n",
    "![confusion matrix](confusion_matrix.png)\n",
    "\n",
    "![convolutional filters](convolutional_filters.png)\n",
    "\n",
    "\n",
    "Image Model (ResNet50) Parameters: 23,508,032\n",
    "\n",
    "Text Model (DistilBERT) Parameters: 66,362,880\n",
    "\n",
    "Total Trainable Parameters: 91,315,268"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492cf241",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f8c560",
   "metadata": {},
   "source": [
    "### Process:\n",
    "Throughout this project we went through many models. Initially, we used a ResNet18 framework for the images, and tried tokenizing the text data from scratch. While this worked, our accuracy was in the ballpark of ~73%. We were not satisfied with this, so we switched to using the DistilBERT model for our text. This improved the model significantly, increasing our accuracy by ~7 percentage points. Once we analized the number of parameters though, we saw that we had ~10,000,000 image parameters using ResNet18, while DistilBERT gave us ~66,000,000 parameters. There are many possible ways to remedy this, but due to time constraints, we went with an upgrade from ResNet18 to ResNet50, knowing the more complex model will give us more parameters. In doing so, we approximately doubled the number of parameters in the image model.\n",
    "While the ratio is still far from perfect, we are satisfied with the results we have currently (given the limitations). \n",
    "\n",
    "### Next Steps:\n",
    "In the future we would like to implement dense layers to equalize the number of parameters contributing from each of the two sources.\n",
    "Additionally, doing more testing around the optimal batch size/number of epochs/learning rate, and finding a good balance between overfitting and underfitting would be a fun endeavour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f5b315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
